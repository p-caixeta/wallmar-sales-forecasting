{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As the exploratory part was already done in the original file, I've cleared it on this version, to jump strainght to the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading datasets\n",
    "features=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\features.csv\")\n",
    "samples=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\sampleSubmission.csv\")\n",
    "stores=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\stores.csv\")\n",
    "test=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\test.csv\")\n",
    "train=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have condensed all modifications on this single cell. For more information or explanation please refere to: \n",
    "# https://github.com/p-caixeta/wallmart-sales-forecasting/blob/master/walmart-recruiting-store-sales-forecasting.ipynb\n",
    "    \n",
    "\n",
    "#getting all information on one singel DF\n",
    "\n",
    "#merging\n",
    "storesComFeatures=pd.merge(stores,features, on=\"Store\",how=\"inner\")\n",
    "\n",
    "#adding sales\n",
    "treino_dados_totais=pd.merge(storesComFeatures,train,how=\"inner\",on=[\"Store\",\"Date\",\"IsHoliday\"]).reset_index(drop=True)\n",
    "treino_dados_totais.sort_values(by=[\"Store\",\"Dept\",\"Date\"])\n",
    "teste_dados_totais=pd.merge(storesComFeatures,test,how=\"inner\",on=[\"Store\",\"Date\",\"IsHoliday\"]).reset_index(drop=True)\n",
    "teste_dados_totais.sort_values(by=[\"Store\",\"Dept\",\"Date\"])\n",
    "\n",
    "#correncting Date and IsHolliday formats\n",
    "treino_dados_totais[\"Date\"]=     pd.to_datetime(treino_dados_totais[\"Date\"])\n",
    "treino_dados_totais[\"IsHoliday\"]=pd.get_dummies(treino_dados_totais[\"IsHoliday\"],drop_first=True)\n",
    "teste_dados_totais[\"Date\"]=      pd.to_datetime(teste_dados_totais[\"Date\"])\n",
    "teste_dados_totais[\"IsHoliday\"]= pd.get_dummies(teste_dados_totais[\"IsHoliday\"],drop_first=True)\n",
    "\n",
    "#creating a \"week\" and a \"year\" column, to follow sales throughout the year\n",
    "treino_dados_totais[\"year\"]=   treino_dados_totais[\"Date\"].dt.year\n",
    "treino_dados_totais[\"week\"]=treino_dados_totais[\"Date\"].dt.week\n",
    "teste_dados_totais[\"year\"]=    teste_dados_totais[\"Date\"].dt.year\n",
    "teste_dados_totais[\"week\"]= teste_dados_totais[\"Date\"].dt.week\n",
    "\n",
    "#getting numeric dummies for store type\n",
    "treinoDummies=pd.get_dummies (treino_dados_totais[\"Type\"],prefix=\"Store_Type\")\n",
    "testeDummies= pd.get_dummies (teste_dados_totais[\"Type\"],prefix=\"Store_Type\")\n",
    "\n",
    "treino_dados_totais=pd.concat([treino_dados_totais, treinoDummies], axis=1)\n",
    "teste_dados_totais= pd.concat([teste_dados_totais, testeDummies], axis=1)\n",
    "\n",
    "treino_dados_totais.drop(\"Type\",axis=1,inplace=True)\n",
    "teste_dados_totais.drop (\"Type\",axis=1,inplace=True)\n",
    "\n",
    "#as Markdowns are discounts, makes sense to fill nulls with 0, since 0 will indicate no discounts\n",
    "treino_dados_totais[\"MarkDown4\"].fillna(0,inplace=True)\n",
    "treino_dados_totais[\"MarkDown5\"].fillna(0,inplace=True)\n",
    "teste_dados_totais[\"MarkDown4\"].fillna(0,inplace=True)\n",
    "teste_dados_totais[\"MarkDown5\"].fillna(0,inplace=True)\n",
    "\n",
    "del storesComFeatures,features,stores,test,train\n",
    "\n",
    "teste_dados_totais.drop([\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"Temperature\",\"CPI\"],axis=1,inplace=True)\n",
    "treino_dados_totais.drop([\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"Temperature\",\"CPI\"],axis=1,inplace=True)\n",
    "\n",
    "teste_dados_totais[\"Unemployment\"].fillna(method=\"ffill\",inplace=True) \n",
    "\n",
    "#removing date\n",
    "teste_final=teste_dados_totais.drop(\"Date\",axis=1)\n",
    "teste_final.sort_values(by=[\"Store\",\"Dept\"],inplace=True)  #to ccorrect the size\n",
    "treino_final=treino_dados_totais.drop(\"Date\",axis=1)\n",
    "\n",
    "#creating WMAE function\n",
    "def WMAE(df, real, pred):\n",
    "    peso = df[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n",
    "    return np.round(np.sum(peso*abs(real-pred))/(np.sum(peso)), 2)\n",
    "\n",
    "xtrain,xtest,ytrain,ytest= train_test_split(\n",
    "    treino_final.drop(\"Weekly_Sales\",axis=1) , treino_final[\"Weekly_Sales\"] , test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method = RF\n"
     ]
    }
   ],
   "source": [
    "#loop for Random Forest:\n",
    "print (\"Method = RF\")\n",
    "min_erro_rf=1665\n",
    "#rf=RandomForestRegressor(n_estimators=n,\n",
    "#                        max_depth=depth,\n",
    "#                        min_samples_split=split,\n",
    "#                        min_samples_leaf=leaf,\n",
    "#                        random_state=42)\n",
    "#rf.fit(xtrain,ytrain)\n",
    "#pred=rf.predict(xtest)\n",
    "#erro=WMAE(xtest,ytest,pred)\n",
    "#print (\"Best result=\",min_erro_rf)\n",
    "#print (\"Best parameters=\",melhor_param_rf)\n",
    "\n",
    "#best \"brute force\" result: WMAE 1665, parameter = 500,100,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the submission\n",
    "#best method: Random Forest\n",
    "#melhor_param_rf = [500, 100, 1, 2]\n",
    "#rf=RandomForestRegressor(n_estimators=melhor_param_rf[0],\n",
    "#                        max_depth=melhor_param_rf[1],\n",
    "#                        min_samples_leaf=melhor_param_rf[2],\n",
    "#                        min_samples_split=melhor_param_rf[3],\n",
    "#                        random_state=42)\n",
    "#rf.fit(xtrain,ytrain)\n",
    "#previsto=rf.predict(teste_final)\n",
    "#samples[\"Weekly_Sales\"]=previsto\n",
    "#samples.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > > **PSO**\n",
    "\n",
    "Now, I'll implement Particle Swarm Optimization (http://ai.unibo.it/sites/ai.unibo.it/files/u11/pso.pdf) to find the best parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim do fit inicial 0 º particula, em 181.65 min total. Fit= 2523.33\n",
      "fim do fit inicial 1 º particula, em 181.81 min total. Fit= 2186.11\n",
      "fim do fit inicial 2 º particula, em 181.85 min total. Fit= 2677.23\n",
      "fim do fit inicial 3 º particula, em 181.90 min total. Fit= 2349.96\n",
      "\n",
      "-----------FIM DA INICIALIZACAO------------------- \n",
      "gbest inicial= 2186 \n",
      "tempo gasto: 0.01 horas \n",
      "tempo medio por fit: 0.09 min \n",
      "xbest: [ 5 16  4  6] \n",
      "----------------------------------------------------\n",
      "Às 2020-05-04 22:12:49.689109 \n",
      "fim da geracao 2 com media de 0.92440 s por fit\n",
      "tempo total: 0.022505422366989984 horas \n",
      "tempo gasto nesta iteracao: 0.31 min\n",
      "gbest ate a geracao 2 = 2186 \n",
      " xbest ate geracao:, 2 \n",
      " [ 5 16  4  6] \n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-4dc72cc1e438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWMAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mgbest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mmelhora\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minte\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-89ce0bc07c48>\u001b[0m in \u001b[0;36mWMAE\u001b[1;34m(df, real, pred)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m#creating WMAE function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mWMAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mpeso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"IsHoliday\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeso\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3591\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3593\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-89ce0bc07c48>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m#creating WMAE function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mWMAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mpeso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"IsHoliday\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeso\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w = 0.729844 # Inertia weight to prevent velocities becoming too large \n",
    "c1 = 2.609 # Scaling co-efficient on the social component\n",
    "c2 = 2.496180 # Scaling co-efficient on the cognitive component\n",
    "iterations = 200\n",
    "swarmSize = 30\n",
    "rd_state=42\n",
    "dimension=4 # 1= n_estimators, 2= max_depth, 3=min_sample_leaf, 4=min_sample_split\n",
    "\n",
    "tempo_inicial=time.time()\n",
    "inicio=time.clock()\n",
    "########### constrains: ##########\n",
    "#     1<= n_estimators <= 1000   #\n",
    "#    10<= max_depth <= 200       #\n",
    "#     1<= min_sample_leaf <=20   #\n",
    "#     1<= min_sample_split <=20  #\n",
    "##################################\n",
    "\n",
    "\n",
    "x=np.zeros((swarmSize,dimension)).astype(int)\n",
    "v=np.zeros((swarmSize,dimension)).astype(int)\n",
    "melhora=np.array(0).astype(int)\n",
    "########## INITIAL POPULATION ################  \n",
    "## n_estimators ##\n",
    "for i in range(swarmSize):\n",
    "    x[i][0]=np.random.randint(1,1000)#1000\n",
    "    v[i][0]=0.01*x[i][0]\n",
    "## max_depth ##\n",
    "    x[i][1]=np.random.randint(10,200)#200\n",
    "    v[i][1]=0.01*x[i][1]\n",
    "## min_sample_leaf ##\n",
    "    x[i][2]=np.random.randint(1,20)\n",
    "    v[i][2]=0.01*x[i][2]\n",
    "## min_sample_split ##\n",
    "    x[i][3]=np.random.randint(1,20)\n",
    "    v[i][3]=0.01*x[i][3]\n",
    "\n",
    "pbest=x.copy()\n",
    "fpbest=np.zeros(swarmSize).astype(int)\n",
    "a=np.zeros(swarmSize).astype(int)\n",
    "b=time.clock()\n",
    "for i in range(swarmSize): \n",
    "    rf=RandomForestRegressor(n_estimators=x[i][0],\n",
    "                        max_depth=x[i][1],\n",
    "                        min_samples_leaf=x[i][2],\n",
    "                        min_samples_split=x[i][3],\n",
    "                        random_state=42)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    pred=rf.predict(xtest)\n",
    "    A=WMAE(xtest,ytest,pred)\n",
    "    fpbest[i]=A.copy()\n",
    "    a[i]=A.copy()\n",
    "    print(\"fim do fit inicial\",i,\"º particula, em\",\"%.2f\" % (time.clock()/60),\"min total. Fit=\",A)\n",
    "a.sort()\n",
    "gbest=a[0].copy()\n",
    "\n",
    "xbest=x[np.where(fpbest==gbest)[0][0]]\n",
    "\n",
    "gbest_inicial=gbest.copy()\n",
    "print (\"\\n-----------FIM DA INICIALIZACAO-------------------\",\n",
    "           \"\\ngbest inicial=\",gbest,\n",
    "           \"\\ntempo gasto:\",\"%.2f\" %((time.time()-tempo_inicial)/3600),\"horas\"\n",
    "           ,\"\\ntempo medio por fit:\",\"%.2f\" %(((time.clock()- b)/swarmSize)/60),\"min\"\n",
    "           ,\"\\nxbest:\",xbest\n",
    "           ,\"\\n----------------------------------------------------\"\n",
    "           \n",
    "           )\n",
    "########## MAIN LOOP #################\n",
    "for inte in range(iterations):\n",
    "    ########## updating generating #################\n",
    "    b=time.time()\n",
    "    for j in range(swarmSize):\n",
    "        r1=np.random.random()            \n",
    "        r2=np.random.random()\n",
    "        social = c1 * r1 * (xbest - x[j])\n",
    "        cognitive = c2 * r2 * (pbest[j] - x[j])\n",
    "        v[j] = (w * v[j]) + social + cognitive\n",
    "                #print (\"aqui\",j,i)\n",
    "            #print (\"aqui2\",j,i)\n",
    "            #print(\"fim da evoucao da\",inte,\" geracao. Durou:\",time.clock()/60,\"min\")\n",
    "        ########## comparaÃ§Ã£o do gbest ####################\n",
    "    for k in range(swarmSize): \n",
    "        c=time.clock()\n",
    "        rf=RandomForestRegressor(n_estimators=x[i][0],\n",
    "                        max_depth=x[i][1],\n",
    "                        min_samples_leaf=x[i][2],\n",
    "                        min_samples_split=x[i][3],\n",
    "                        random_state=42)\n",
    "        rf.fit(xtrain,ytrain)\n",
    "        pred=rf.predict(xtest)\n",
    "        A=WMAE(xtest,ytest,pred)\n",
    "        if A<gbest:\n",
    "            melhora+=[inte]\n",
    "            gbest=A.copy()\n",
    "            xbest=x[k].copy()\n",
    "        if A<fpbest[k]:\n",
    "            fpbest[k]=A.copy()\n",
    "            pbest[k]=x[k].copy()\n",
    "        else:\n",
    "            pass\n",
    "    d=time.clock()\n",
    "    if inte%10==0 and inte>1:\n",
    "        print(\"Às\",datetime.now(),\n",
    "            \"\\nfim da geracao\",inte,\"com media de\",\"%.5f\" % ((d-c)/swarmSize),\n",
    "                  \"s por fit\")\n",
    "\n",
    "        print (\"tempo total:\",(time.time()-tempo_inicial)/3600,\"horas\",#)\n",
    "                   \"\\ntempo gasto nesta iteracao:\",\"%.2f\" %((time.time()-b)/60),\"min\")\n",
    "\n",
    "        print (\"gbest ate a geracao\",inte,\"=\",gbest,\n",
    "                   \"\\n xbest ate geracao:,\",inte,\"\\n\",xbest,\n",
    "                   \"\\n-----------------------\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
