{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As the exploratory part was already done in the original file, I've cleared it on this version, to jump strainght to the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading datasets\n",
    "features=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\features.csv\")\n",
    "samples=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\sampleSubmission.csv\")\n",
    "stores=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\stores.csv\")\n",
    "test=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\test.csv\")\n",
    "train=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have condensed all modifications on this single cell. For more information or explanation please refere to: \n",
    "# https://github.com/p-caixeta/wallmart-sales-forecasting/blob/master/walmart-recruiting-store-sales-forecasting.ipynb\n",
    "    \n",
    "\n",
    "#getting all information on one singel DF\n",
    "\n",
    "#merging\n",
    "storesComFeatures=pd.merge(stores,features, on=\"Store\",how=\"inner\")\n",
    "\n",
    "#adding sales\n",
    "treino_dados_totais=pd.merge(storesComFeatures,train,how=\"inner\",on=[\"Store\",\"Date\",\"IsHoliday\"]).reset_index(drop=True)\n",
    "treino_dados_totais.sort_values(by=[\"Store\",\"Dept\",\"Date\"])\n",
    "teste_dados_totais=pd.merge(storesComFeatures,test,how=\"inner\",on=[\"Store\",\"Date\",\"IsHoliday\"]).reset_index(drop=True)\n",
    "teste_dados_totais.sort_values(by=[\"Store\",\"Dept\",\"Date\"])\n",
    "\n",
    "#correncting Date and IsHolliday formats\n",
    "treino_dados_totais[\"Date\"]=     pd.to_datetime(treino_dados_totais[\"Date\"])\n",
    "treino_dados_totais[\"IsHoliday\"]=pd.get_dummies(treino_dados_totais[\"IsHoliday\"],drop_first=True)\n",
    "teste_dados_totais[\"Date\"]=      pd.to_datetime(teste_dados_totais[\"Date\"])\n",
    "teste_dados_totais[\"IsHoliday\"]= pd.get_dummies(teste_dados_totais[\"IsHoliday\"],drop_first=True)\n",
    "\n",
    "#creating a \"week\" and a \"year\" column, to follow sales throughout the year\n",
    "treino_dados_totais[\"year\"]=   treino_dados_totais[\"Date\"].dt.year\n",
    "treino_dados_totais[\"week\"]=treino_dados_totais[\"Date\"].dt.week\n",
    "teste_dados_totais[\"year\"]=    teste_dados_totais[\"Date\"].dt.year\n",
    "teste_dados_totais[\"week\"]= teste_dados_totais[\"Date\"].dt.week\n",
    "\n",
    "#getting numeric dummies for store type\n",
    "treinoDummies=pd.get_dummies (treino_dados_totais[\"Type\"],prefix=\"Store_Type\")\n",
    "testeDummies= pd.get_dummies (teste_dados_totais[\"Type\"],prefix=\"Store_Type\")\n",
    "\n",
    "treino_dados_totais=pd.concat([treino_dados_totais, treinoDummies], axis=1)\n",
    "teste_dados_totais= pd.concat([teste_dados_totais, testeDummies], axis=1)\n",
    "\n",
    "treino_dados_totais.drop(\"Type\",axis=1,inplace=True)\n",
    "teste_dados_totais.drop (\"Type\",axis=1,inplace=True)\n",
    "\n",
    "#as Markdowns are discounts, makes sense to fill nulls with 0, since 0 will indicate no discounts\n",
    "treino_dados_totais[\"MarkDown4\"].fillna(0,inplace=True)\n",
    "treino_dados_totais[\"MarkDown5\"].fillna(0,inplace=True)\n",
    "teste_dados_totais[\"MarkDown4\"].fillna(0,inplace=True)\n",
    "teste_dados_totais[\"MarkDown5\"].fillna(0,inplace=True)\n",
    "\n",
    "del storesComFeatures,features,stores,test,train\n",
    "\n",
    "teste_dados_totais.drop([\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"Temperature\",\"CPI\"],axis=1,inplace=True)\n",
    "treino_dados_totais.drop([\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"Temperature\",\"CPI\"],axis=1,inplace=True)\n",
    "\n",
    "teste_dados_totais[\"Unemployment\"].fillna(method=\"ffill\",inplace=True) \n",
    "\n",
    "#removing date\n",
    "teste_final=teste_dados_totais.drop(\"Date\",axis=1)\n",
    "teste_final.sort_values(by=[\"Store\",\"Dept\"],inplace=True)  #to ccorrect the size\n",
    "treino_final=treino_dados_totais.drop(\"Date\",axis=1)\n",
    "\n",
    "#creating WMAE function\n",
    "def WMAE(df, real, pred):\n",
    "    peso = df[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n",
    "    return np.round(np.sum(peso*abs(real-pred))/(np.sum(peso)), 2)\n",
    "\n",
    "xtrain,xtest,ytrain,ytest= train_test_split(\n",
    "    treino_final.drop(\"Weekly_Sales\",axis=1) , treino_final[\"Weekly_Sales\"] , test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method = RF\n"
     ]
    }
   ],
   "source": [
    "#loop for Random Forest:\n",
    "print (\"Method = RF\")\n",
    "min_erro_rf=1665\n",
    "#rf=RandomForestRegressor(n_estimators=n,\n",
    "#                        max_depth=depth,\n",
    "#                        min_samples_split=split,\n",
    "#                        min_samples_leaf=leaf,\n",
    "#                        random_state=42)\n",
    "#rf.fit(xtrain,ytrain)\n",
    "#pred=rf.predict(xtest)\n",
    "#erro=WMAE(xtest,ytest,pred)\n",
    "#print (\"Best result=\",min_erro_rf)\n",
    "#print (\"Best parameters=\",melhor_param_rf)\n",
    "\n",
    "#best \"brute force\" result: WMAE 1665, parameter = 500,100,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the submission\n",
    "#best method: Random Forest\n",
    "#melhor_param_rf = [500, 100, 1, 2]\n",
    "#rf=RandomForestRegressor(n_estimators=melhor_param_rf[0],\n",
    "#                        max_depth=melhor_param_rf[1],\n",
    "#                        min_samples_leaf=melhor_param_rf[2],\n",
    "#                        min_samples_split=melhor_param_rf[3],\n",
    "#                        random_state=42)\n",
    "#rf.fit(xtrain,ytrain)\n",
    "#previsto=rf.predict(teste_final)\n",
    "#samples[\"Weekly_Sales\"]=previsto\n",
    "#samples.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > > **PSO**\n",
    "\n",
    "Now, I'll implement Particle Swarm Optimization (http://ai.unibo.it/sites/ai.unibo.it/files/u11/pso.pdf) to find the best parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim da pop inicial\n"
     ]
    }
   ],
   "source": [
    "w = 0.729844 # Inertia weight to prevent velocities becoming too large \n",
    "c1 = 2.609 # Scaling co-efficient on the social component\n",
    "c2 = 2.496180 # Scaling co-efficient on the cognitive component\n",
    "iterations = 200\n",
    "swarmSize = 200\n",
    "rd_state=42\n",
    "dimension=4 # 1= n_estimators, 2= max_depth, 3=min_sample_leaf, 4=min_sample_split\n",
    "\n",
    "tempo_inicial=time.time()\n",
    "inicio=time.clock()\n",
    "## constrains: ##\n",
    "#     1<= n_estimators <= 1000 \n",
    "#    10<= max_depth <= 200\n",
    "#     1<= min_sample_leaf <=20\n",
    "#     1<= min_sample_split <=20\n",
    "\n",
    "y=[0]*dimension\n",
    "x=[0]*swarmSize\n",
    "v=[0]*swarmSize\n",
    "for i in range (swarmSize):\n",
    "    x[i]=copy.deepcopy(y)\n",
    "    v[i]=copy.deepcopy(y)\n",
    "melhora=[]\n",
    "########## INITIAL POPULATION ################  \n",
    "## n_estimators ##\n",
    "for i in range(swarmSize):\n",
    "    x[i][0]=np.random.randint(1,1000)\n",
    "    v[i][0]=0.01*x[i][0]\n",
    "## max_depth ##\n",
    "    x[i][1]=np.random.randint(10,200)\n",
    "    v[i][1]=0.01*x[i][1]\n",
    "## min_sample_leaf ##\n",
    "    x[i][2]=np.random.randint(1,20)\n",
    "    v[i][2]=0.01*x[i][2]\n",
    "## min_sample_split ##\n",
    "    x[i][3]=np.random.randint(1,20)\n",
    "    v[i][3]=0.01*x[i][3]\n",
    "\n",
    "pbest=copy.deepcopy(x) \n",
    "fpbest=[None]*swarmSize\n",
    "a=[None]*swarmSize\n",
    "b=time.clock()\n",
    "for i in range(swarmSize): \n",
    "    rf=RandomForestRegressor(n_estimators=x[i][0],\n",
    "                        max_depth=x[i][1],\n",
    "                        min_samples_leaf=x[i][2],\n",
    "                        min_samples_split=x[i][3],\n",
    "                        random_state=42)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    A=rf.predict(teste_final)\n",
    "    fpbest[i]=copy.deepcopy([A])\n",
    "    a[i]=copy.deepcopy([A])\n",
    "    print(\"fim do fit inicial\",i,\"particula\",time.clock())\n",
    "a.sort()\n",
    "gbest=copy.deepcopy(a[0])\n",
    "\n",
    "xbest=copy.deepcopy(x[fpbest.index(gbest)])\n",
    "\n",
    "gbest_inicial=copy.deepcopy(gbest[0])\n",
    "print (\"\\n-----------FIM DA INICIALIZACAO-------------------\",\n",
    "           \"\\ngbest inicial=\",gbest[0],\n",
    "           \"\\ntempo gasto:\",time.time()-tempo_inicial,\"s\"\n",
    "           ,\"\\ntempo medio por fit:\",(time.clock()- b)/swarmSize,\n",
    "           \"s\",\"\\n----------------------------------------------------\"\n",
    "           ,\"xbest:\\n\",xbest\n",
    "           )\n",
    "########## MAIN LOOP #################\n",
    "for inte in range(iterations):\n",
    "    ########## updating generating #################\n",
    "    b=time.time()\n",
    "    for j in range(swarmSize):\n",
    "        r1=np.random.random()            \n",
    "        r2=np.random.random()\n",
    "        social = c1 * r1 * (xbest - x[j])\n",
    "        cognitive = c2 * r2 * (pbest[j] - x[j])\n",
    "        v[j] = (w * v[j]) + social + cognitive\n",
    "                #print (\"aqui\",j,i)\n",
    "            #print (\"aqui2\",j,i)\n",
    "            #print(\"fim da evoucao da\",inte,\" geracao. Durou:\",time.clock()/60,\"min\")\n",
    "        ########## comparaÃ§Ã£o do gbest ####################\n",
    "    for k in range(swarmSize): \n",
    "        c=time.clock()\n",
    "        rf=RandomForestRegressor(n_estimators=x[i][0],\n",
    "                        max_depth=x[i][1],\n",
    "                        min_samples_leaf=x[i][2],\n",
    "                        min_samples_split=x[i][3],\n",
    "                        random_state=42)\n",
    "        rf.fit(xtrain,ytrain)\n",
    "        A=rf.predict(teste_final)\n",
    "        if A<gbest:\n",
    "            melhora+=[inte]\n",
    "            gbest=copy.deepcopy(A)\n",
    "            xbest=copy.deepcopy(x[k])\n",
    "        if A<fpbest[k]:\n",
    "            fpbest[k]=copy.deepcopy(A)\n",
    "            pbest[k]=copy.deepcopy(x[k])\n",
    "        else:\n",
    "            pass\n",
    "    d=time.clock()\n",
    "    if inte%10==0 and inte>1:\n",
    "        print(\"\\nfim da geracao\",inte,\"com media de\",\"%.5f\" % ((d-c)/swarmSize),\n",
    "                  \"s por fit\")\n",
    "\n",
    "        print (\"tempo total:\",(time.time()-tempo_inicial)/60,\"min\",#)\n",
    "                   \"\\ntempo gasto nesta iteracao:\",time.time()-b,\"s\")\n",
    "\n",
    "        print (\"gbest ate a geracao\",inte,\"=\",gbest,\n",
    "                   \"\\n xbest ate geracao:\\n\",xbest,\n",
    "                   \"\\n-----------------------\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
