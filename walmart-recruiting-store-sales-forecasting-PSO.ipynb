{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As the exploratory part was already done in the original file, I've cleared it on this version, to jump strainght to the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading datasets\n",
    "features=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\features.csv\")\n",
    "samples=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\sampleSubmission.csv\")\n",
    "stores=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\stores.csv\")\n",
    "test=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\test.csv\")\n",
    "train=pd.read_csv(\"C:\\\\Users\\\\Paulo\\\\Desktop\\\\walmart-git\\\\wallmart-sales-forecasting\\\\train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have condensed all modifications on this single cell. For more information or explanation please refere to: \n",
    "# https://github.com/p-caixeta/wallmart-sales-forecasting/blob/master/walmart-recruiting-store-sales-forecasting.ipynb\n",
    "    \n",
    "\n",
    "#getting all information on one singel DF\n",
    "\n",
    "#merging\n",
    "storesComFeatures=pd.merge(stores,features, on=\"Store\",how=\"inner\")\n",
    "\n",
    "#adding sales\n",
    "treino_dados_totais=pd.merge(storesComFeatures,train,how=\"inner\",on=[\"Store\",\"Date\",\"IsHoliday\"]).reset_index(drop=True)\n",
    "treino_dados_totais.sort_values(by=[\"Store\",\"Dept\",\"Date\"])\n",
    "teste_dados_totais=pd.merge(storesComFeatures,test,how=\"inner\",on=[\"Store\",\"Date\",\"IsHoliday\"]).reset_index(drop=True)\n",
    "teste_dados_totais.sort_values(by=[\"Store\",\"Dept\",\"Date\"])\n",
    "\n",
    "#correncting Date and IsHolliday formats\n",
    "treino_dados_totais[\"Date\"]=     pd.to_datetime(treino_dados_totais[\"Date\"])\n",
    "treino_dados_totais[\"IsHoliday\"]=pd.get_dummies(treino_dados_totais[\"IsHoliday\"],drop_first=True)\n",
    "teste_dados_totais[\"Date\"]=      pd.to_datetime(teste_dados_totais[\"Date\"])\n",
    "teste_dados_totais[\"IsHoliday\"]= pd.get_dummies(teste_dados_totais[\"IsHoliday\"],drop_first=True)\n",
    "\n",
    "#creating a \"week\" and a \"year\" column, to follow sales throughout the year\n",
    "treino_dados_totais[\"year\"]=   treino_dados_totais[\"Date\"].dt.year\n",
    "treino_dados_totais[\"week\"]=treino_dados_totais[\"Date\"].dt.week\n",
    "teste_dados_totais[\"year\"]=    teste_dados_totais[\"Date\"].dt.year\n",
    "teste_dados_totais[\"week\"]= teste_dados_totais[\"Date\"].dt.week\n",
    "\n",
    "#getting numeric dummies for store type\n",
    "treinoDummies=pd.get_dummies (treino_dados_totais[\"Type\"],prefix=\"Store_Type\")\n",
    "testeDummies= pd.get_dummies (teste_dados_totais[\"Type\"],prefix=\"Store_Type\")\n",
    "\n",
    "treino_dados_totais=pd.concat([treino_dados_totais, treinoDummies], axis=1)\n",
    "teste_dados_totais= pd.concat([teste_dados_totais, testeDummies], axis=1)\n",
    "\n",
    "treino_dados_totais.drop(\"Type\",axis=1,inplace=True)\n",
    "teste_dados_totais.drop (\"Type\",axis=1,inplace=True)\n",
    "\n",
    "#as Markdowns are discounts, makes sense to fill nulls with 0, since 0 will indicate no discounts\n",
    "treino_dados_totais[\"MarkDown4\"].fillna(0,inplace=True)\n",
    "treino_dados_totais[\"MarkDown5\"].fillna(0,inplace=True)\n",
    "teste_dados_totais[\"MarkDown4\"].fillna(0,inplace=True)\n",
    "teste_dados_totais[\"MarkDown5\"].fillna(0,inplace=True)\n",
    "\n",
    "del storesComFeatures,features,stores,test,train\n",
    "\n",
    "teste_dados_totais.drop([\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"Temperature\",\"CPI\"],axis=1,inplace=True)\n",
    "treino_dados_totais.drop([\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"Temperature\",\"CPI\"],axis=1,inplace=True)\n",
    "\n",
    "teste_dados_totais[\"Unemployment\"].fillna(method=\"ffill\",inplace=True) \n",
    "\n",
    "#removing date\n",
    "teste_final=teste_dados_totais.drop(\"Date\",axis=1)\n",
    "teste_final.sort_values(by=[\"Store\",\"Dept\"],inplace=True)  #to ccorrect the size\n",
    "treino_final=treino_dados_totais.drop(\"Date\",axis=1)\n",
    "\n",
    "#creating WMAE function\n",
    "def WMAE(df, real, pred):\n",
    "    peso = df[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n",
    "    return np.round(np.sum(peso*abs(real-pred))/(np.sum(peso)), 2)\n",
    "\n",
    "xtrain,xtest,ytrain,ytest= train_test_split(\n",
    "    treino_final.drop(\"Weekly_Sales\",axis=1) , treino_final[\"Weekly_Sales\"] , test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method = RF\n"
     ]
    }
   ],
   "source": [
    "#loop for Random Forest:\n",
    "print (\"Method = RF\")\n",
    "min_erro_rf=1665\n",
    "#rf=RandomForestRegressor(n_estimators=n,\n",
    "#                        max_depth=depth,\n",
    "#                        min_samples_split=split,\n",
    "#                        min_samples_leaf=leaf,\n",
    "#                        random_state=42)\n",
    "#rf.fit(xtrain,ytrain)\n",
    "#pred=rf.predict(xtest)\n",
    "#erro=WMAE(xtest,ytest,pred)\n",
    "#print (\"Best result=\",min_erro_rf)\n",
    "#print (\"Best parameters=\",melhor_param_rf)\n",
    "\n",
    "#best \"brute force\" result: WMAE 1665, parameter = 500,100,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the submission\n",
    "#best method: Random Forest\n",
    "#melhor_param_rf = [500, 100, 1, 2]\n",
    "#rf=RandomForestRegressor(n_estimators=melhor_param_rf[0],\n",
    "#                        max_depth=melhor_param_rf[1],\n",
    "#                        min_samples_leaf=melhor_param_rf[2],\n",
    "#                        min_samples_split=melhor_param_rf[3],\n",
    "#                        random_state=42)\n",
    "#rf.fit(xtrain,ytrain)\n",
    "#previsto=rf.predict(teste_final)\n",
    "#samples[\"Weekly_Sales\"]=previsto\n",
    "#samples.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# > > **PSO**\n",
    "\n",
    "Now, I'll implement Particle Swarm Optimization (http://ai.unibo.it/sites/ai.unibo.it/files/u11/pso.pdf) to find the best parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_estimators must be an integer, got <class 'numpy.float64'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-538079e7498b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m                         \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                         random_state=42)\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mteste_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mfpbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbootstrap\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\base.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self, default)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             raise ValueError(\"n_estimators must be an integer, \"\n\u001b[1;32m--> 106\u001b[1;33m                              \"got {0}.\".format(type(self.n_estimators)))\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: n_estimators must be an integer, got <class 'numpy.float64'>."
     ]
    }
   ],
   "source": [
    "w = 0.729844 # Inertia weight to prevent velocities becoming too large \n",
    "c1 = 2.609 # Scaling co-efficient on the social component\n",
    "c2 = 2.496180 # Scaling co-efficient on the cognitive component\n",
    "iterations = 200\n",
    "swarmSize = 5\n",
    "rd_state=42\n",
    "dimension=4 # 1= n_estimators, 2= max_depth, 3=min_sample_leaf, 4=min_sample_split\n",
    "\n",
    "tempo_inicial=time.time()\n",
    "inicio=time.clock()\n",
    "########### constrains: ##########\n",
    "#     1<= n_estimators <= 1000   #\n",
    "#    10<= max_depth <= 200       #\n",
    "#     1<= min_sample_leaf <=20   #\n",
    "#     1<= min_sample_split <=20  #\n",
    "##################################\n",
    "\n",
    "\n",
    "x=np.zeros((swarmSize,dimension)).astype(int)\n",
    "v=np.zeros((swarmSize,dimension)).astype(int)\n",
    "melhora=np.array(0).astype(int)\n",
    "########## INITIAL POPULATION ################  \n",
    "## n_estimators ##\n",
    "for i in range(swarmSize):\n",
    "    x[i][0]=np.random.randint(1,1000)\n",
    "    v[i][0]=0.01*x[i][0]\n",
    "## max_depth ##\n",
    "    x[i][1]=np.random.randint(10,200)\n",
    "    v[i][1]=0.01*x[i][1]\n",
    "## min_sample_leaf ##\n",
    "    x[i][2]=np.random.randint(1,20)\n",
    "    v[i][2]=0.01*x[i][2]\n",
    "## min_sample_split ##\n",
    "    x[i][3]=np.random.randint(1,20)\n",
    "    v[i][3]=0.01*x[i][3]\n",
    "\n",
    "pbest=x.copy()\n",
    "fpbest=np.zeros(swarmSize)\n",
    "a=np.zeros(swarmSize)\n",
    "b=time.clock()\n",
    "for i in range(swarmSize): \n",
    "    rf=RandomForestRegressor(n_estimators=x[i][0],\n",
    "                        max_depth=x[i][1],\n",
    "                        min_samples_leaf=x[i][2],\n",
    "                        min_samples_split=x[i][3],\n",
    "                        random_state=42)\n",
    "    rf.fit(xtrain,ytrain)\n",
    "    A=rf.predict(teste_final)\n",
    "    fpbest[i]=A.copy()\n",
    "    a[i]=A.copy()\n",
    "    print(\"fim do fit inicial\",i,\"º particula, em\",time.clock()/60,\"min\")\n",
    "a.sort()\n",
    "gbest=a[0].copy()\n",
    "\n",
    "xbest=x[fpbest.index(gbest)].copy()\n",
    "\n",
    "gbest_inicial=gbest[0].copy()\n",
    "print (\"\\n-----------FIM DA INICIALIZACAO-------------------\",\n",
    "           \"\\ngbest inicial=\",gbest[0],\n",
    "           \"\\ntempo gasto:\",time.time()-tempo_inicial,\"s\"\n",
    "           ,\"\\ntempo medio por fit:\",(time.clock()- b)/swarmSize,\n",
    "           \"s\",\"\\n----------------------------------------------------\"\n",
    "           ,\"xbest:\\n\",xbest\n",
    "           )\n",
    "########## MAIN LOOP #################\n",
    "for inte in range(iterations):\n",
    "    ########## updating generating #################\n",
    "    b=time.time()\n",
    "    for j in range(swarmSize):\n",
    "        r1=np.random.random()            \n",
    "        r2=np.random.random()\n",
    "        social = c1 * r1 * (xbest - x[j])\n",
    "        cognitive = c2 * r2 * (pbest[j] - x[j])\n",
    "        v[j] = (w * v[j]) + social + cognitive\n",
    "                #print (\"aqui\",j,i)\n",
    "            #print (\"aqui2\",j,i)\n",
    "            #print(\"fim da evoucao da\",inte,\" geracao. Durou:\",time.clock()/60,\"min\")\n",
    "        ########## comparaÃ§Ã£o do gbest ####################\n",
    "    for k in range(swarmSize): \n",
    "        c=time.clock()\n",
    "        rf=RandomForestRegressor(n_estimators=x[i][0],\n",
    "                        max_depth=x[i][1],\n",
    "                        min_samples_leaf=x[i][2],\n",
    "                        min_samples_split=x[i][3],\n",
    "                        random_state=42)\n",
    "        rf.fit(xtrain,ytrain)\n",
    "        A=rf.predict(teste_final)\n",
    "        if A<gbest:\n",
    "            melhora+=[inte]\n",
    "            gbest=A.copy()\n",
    "            xbest=x[k].copy()\n",
    "        if A<fpbest[k]:\n",
    "            fpbest[k]=A.copy()\n",
    "            pbest[k]=x[k].copy()\n",
    "        else:\n",
    "            pass\n",
    "    d=time.clock()\n",
    "    if inte%10==0 and inte>1:\n",
    "        print(\"\\nfim da geracao\",inte,\"com media de\",\"%.5f\" % ((d-c)/swarmSize),\n",
    "                  \"s por fit\")\n",
    "\n",
    "        print (\"tempo total:\",(time.time()-tempo_inicial)/60,\"min\",#)\n",
    "                   \"\\ntempo gasto nesta iteracao:\",time.time()-b,\"s\")\n",
    "\n",
    "        print (\"gbest ate a geracao\",inte,\"=\",gbest,\n",
    "                   \"\\n xbest ate geracao:\\n\",xbest,\n",
    "                   \"\\n-----------------------\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[283.,  88.,  13.,   2.],\n",
       "       [943.,  10.,   4.,   3.],\n",
       "       [473.,  57.,   8.,  19.],\n",
       "       [244.,  34.,  14.,  14.],\n",
       "       [274.,  28.,  13.,  17.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
